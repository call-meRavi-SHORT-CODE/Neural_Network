{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "##  Neural Networks\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "- Implementing a neural network with a single perceptron and two input nodes for linear regression\n",
    "- Implementing forward propagation using matrix multiplication\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "###  Linear Regression\n",
    "\n",
    "\n",
    "**Linear regression** is a linear approach for modelling the relationship between a scalar response (**dependent variable**) and one or more explanatory variables (**independent variables**). You will work with a linear regression with $2$ independent variables.\n",
    "\n",
    "Linear regression model with two independent variables $x_1$, $x_2$ can be written as\n",
    "\n",
    "$$\\hat{y} = w_1x_1 + w_2x_2 + b = Wx + b,\\tag{6}$$\n",
    "\n",
    "where $Wx$ is the dot product of the input vector $x = \\begin{bmatrix} x_1 & x_2\\end{bmatrix}$ and parameters vector $W = \\begin{bmatrix} w_1 & w_2\\end{bmatrix}$, scalar parameter $b$ is the intercept. \n",
    "\n",
    "The goal is the same - find the \"best\" parameters $w_1$, $w_2$ and $b$ such the differences between original values $y_i$ and predicted values $\\hat{y}_i$ are minimum.\n",
    "\n",
    "We can use a neural network model to do that. Matrix multiplication will be in the core of the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "###  Neural Network Model with a Single Perceptron and Two Input Nodes\n",
    "\n",
    "Again, you will use only one perceptron, but with two input nodes shown in the following scheme:\n",
    "\n",
    "\n",
    "The perceptron output calculation for a training example $x = \\begin{bmatrix} x_1& x_2\\end{bmatrix}$ can be written with dot product:\n",
    "\n",
    "$$z = w_1x_1 + w_2x_2+ b = Wx + b$$\n",
    "\n",
    "where weights are in the vector $W = \\begin{bmatrix} w_1 & w_2\\end{bmatrix}$ and bias $b$ is a scalar. The output layer will have the same single node $\\hat{y}= z$.\n",
    "\n",
    "Organise all training examples in a matrix $X$ of a shape ($2 \\times m$), putting $x_1$ and $x_2$ into columns. Then matrix multiplication of $W$ ($1 \\times 2$) and $X$ ($2 \\times m$) will give a ($1 \\times m$) vector\n",
    "\n",
    "$$WX = \n",
    "\\begin{bmatrix} w_1 & w_2\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "x_1^{(1)} & x_1^{(2)} & \\dots & x_1^{(m)} \\\\ \n",
    "x_2^{(1)} & x_2^{(2)} & \\dots & x_2^{(m)} \\\\ \\end{bmatrix}\n",
    "=\\begin{bmatrix} \n",
    "w_1x_1^{(1)} + w_2x_2^{(1)} & \n",
    "w_1x_1^{(2)} + w_2x_2^{(2)} & \\dots & \n",
    "w_1x_1^{(m)} + w_2x_2^{(m)}\\end{bmatrix}.$$\n",
    "\n",
    "And the model can be written as\n",
    "\n",
    "\\begin{align}\n",
    "Z &=  W X + b,\\\\\n",
    "\\hat{Y} &= Z,\n",
    "\\tag{8}\\end{align}\n",
    "\n",
    "where $b$ is broadcasted to the vector of a size ($1 \\times m$). These are the calculations to perform in the forward propagation step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can compare the resulting vector of the predictions $\\hat{Y}$ ($1 \\times m$) with the original vector of data $Y$. This can be done with the so called **cost function** that measures how close your vector of predictions is to the training data. It evaluates how well the parameters $w$ and $b$ work to solve the problem. There are many different cost functions available depending on the nature of your problem. For your simple neural network you can calculate it as:\n",
    "\n",
    "$$\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.\\tag{5}$$\n",
    "\n",
    "The aim is to minimize the cost function during the training, which will minimize the differences between original values $y_i$ and predicted values $\\hat{y}_i$ (division by $2m$ is taken just for scaling purposes).\n",
    "\n",
    "When your weights were just initialized with some random values, and no training was done yet, you can't expect good results.\n",
    "\n",
    "The next step is to adjust the weights and bias, in order to minimize the cost function. This process is called **backward propagation** and is done iteratively: you update the parameters with a small change and repeat the process.\n",
    "\n",
    "*Note*: Backward propagation is not covered in this Course - it will be discussed in the next Course of this Specialization.\n",
    "\n",
    "The general **methodology** to build a neural network is to:\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Implement forward propagation (calculate the perceptron output),\n",
    "    - Implement backward propagation (to get the required corrections for the parameters),\n",
    "    - Update parameters.\n",
    "4. Make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "###  Parameters of the Neural Network\n",
    "\n",
    "The neural network you will be working with has $3$ parameters. Two weights and one bias, you will start initalizing these parameters as some random numbers, so the algorithm can start at some point. The parameters will be stored in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.4\"></a>\n",
    "###  1)  Forward propagation\n",
    "\n",
    "\n",
    "\n",
    "Implement `forward_propagation()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Look at the mathematical representation of your model:\n",
    "\\begin{align}\n",
    "Z &=  W X + b\\\\\n",
    "\\hat{Y} &= Z,\n",
    "\\end{align}\n",
    "- The steps you have to implement are:\n",
    "    1. Retrieve each parameter from the dictionary \"parameters\" by using `parameters[\"..\"]`.\n",
    "    2. Implement Forward Propagation. Compute `Z` multiplying arrays `W`, `X` and adding vector `b`. Set the prediction array $A$ equal to $Z$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m), where n_x is the dimension input (in our example is 2) and m is the number of training samples\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat -- The output of size (1, m)\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\".\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Implement Forward Propagation to calculate Z.\n",
    "    ### START CODE HERE ### (~ 2 lines of code)\n",
    "    Z = np.dot(W,X)+b\n",
    "    Y_hat = Z\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "### 2) Defining the cost function\n",
    "\n",
    "The cost function used to traing this model is \n",
    "\n",
    "$$\\mathcal{L}\\left(w, b\\right)  = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$\n",
    "\n",
    "The next implementation is not graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_cost(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost function as a sum of squares\n",
    "    \n",
    "    Arguments:\n",
    "    Y_hat -- The output of the neural network of shape (n_y, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (n_y, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- sum of squares scaled by 1/(2*number of examples)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Number of examples.\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute the cost function.\n",
    "    cost = np.sum((Y_hat - Y)**2)/(2*m)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "### 3) Initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x):\n",
    "    \"\"\"\n",
    "    Initializes parameters for a single-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    n_x -- size of the input layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing parameters:\n",
    "                    W -- weight matrix of shape (1, n_x)\n",
    "                    b -- bias scalar\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    W = np.random.randn(1, n_x) * 0.01\n",
    "    b = np.zeros((1, 1))\n",
    "    parameters = {\"W\": W, \"b\": b}\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "### 4) Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(parameters, Y_hat, X, Y, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Update the parameters using gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters\n",
    "    Y_hat -- The output of the neural network of shape (1, m)\n",
    "    X -- input data of shape (n_x, m)\n",
    "    Y -- \"true\" labels vector of shape (1, m)\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "\n",
    "    Returns:\n",
    "    parameters -- updated parameters\n",
    "    \"\"\"\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # Compute gradients\n",
    "    dZ = Y_hat - Y\n",
    "    dW = np.dot(dZ, X.T) / m\n",
    "    db = np.sum(dZ) / m\n",
    "\n",
    "    # Update parameters\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "\n",
    "    parameters = {\"W\": W, \"b\": b}\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex06\"></a>\n",
    "### 5) Implementing the Neural Network\n",
    "\n",
    "Now you're ready to implement your neural network. The next function will implement the training process and it will return the updated parameters dictionary where you will be able to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, num_iterations=1000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (n_x, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations in the loop\n",
    "    print_cost -- if True, print the cost every iteration\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x) \n",
    "    \n",
    "    # Loop\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (~ 2 lines of code)\n",
    "        Y_hat = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"Y_hat, Y\". Outputs: \"cost\".\n",
    "        cost = compute_cost(Y_hat, Y)\n",
    "        \n",
    "        \n",
    "        # Parameters update.\n",
    "        parameters = train_nn(parameters, Y_hat, X, Y, learning_rate = 0.001) \n",
    "        \n",
    "        # Print the cost every iteration.\n",
    "        if print_cost:\n",
    "            if i%100 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C://Users//Ravikrishna J//OneDrive//Desktop//NN//toy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.624345</td>\n",
       "      <td>-1.719394</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.611756</td>\n",
       "      <td>0.057121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.528172</td>\n",
       "      <td>-0.799547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.072969</td>\n",
       "      <td>-0.291595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.865408</td>\n",
       "      <td>-0.258983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.828628</td>\n",
       "      <td>-0.116444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.528880</td>\n",
       "      <td>-2.277298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-2.237087</td>\n",
       "      <td>-0.069625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-1.107713</td>\n",
       "      <td>0.353870</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.017718</td>\n",
       "      <td>-0.186955</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2  y\n",
       "0    1.624345 -1.719394  0\n",
       "1   -0.611756  0.057121  0\n",
       "2   -0.528172 -0.799547  0\n",
       "3   -1.072969 -0.291595  0\n",
       "4    0.865408 -0.258983  0\n",
       "..        ...       ... ..\n",
       "495 -0.828628 -0.116444  0\n",
       "496  0.528880 -2.277298  0\n",
       "497 -2.237087 -0.069625  0\n",
       "498 -1.107713  0.353870  1\n",
       "499 -0.017718 -0.186955  0\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first turn the data into a numpy array to pass it to the our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.624345</td>\n",
       "      <td>-1.719394</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.611756</td>\n",
       "      <td>0.057121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.528172</td>\n",
       "      <td>-0.799547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.072969</td>\n",
       "      <td>-0.291595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.865408</td>\n",
       "      <td>-0.258983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.828628</td>\n",
       "      <td>-0.116444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.528880</td>\n",
       "      <td>-2.277298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-2.237087</td>\n",
       "      <td>-0.069625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-1.107713</td>\n",
       "      <td>0.353870</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.017718</td>\n",
       "      <td>-0.186955</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2  y\n",
       "0    1.624345 -1.719394  0\n",
       "1   -0.611756  0.057121  0\n",
       "2   -0.528172 -0.799547  0\n",
       "3   -1.072969 -0.291595  0\n",
       "4    0.865408 -0.258983  0\n",
       "..        ...       ... ..\n",
       "495 -0.828628 -0.116444  0\n",
       "496  0.528880 -2.277298  0\n",
       "497 -2.237087 -0.069625  0\n",
       "498 -1.107713  0.353870  1\n",
       "499 -0.017718 -0.186955  0\n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(df[['x1','x2']]).T\n",
    "Y = np.array(df['y']).reshape(1,-1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next block to update the parameters dictionary with the fitted weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.269763\n",
      "Cost after iteration 100: 0.229133\n",
      "Cost after iteration 200: 0.196193\n",
      "Cost after iteration 300: 0.169488\n",
      "Cost after iteration 400: 0.147836\n",
      "Cost after iteration 500: 0.130282\n",
      "Cost after iteration 600: 0.116048\n",
      "Cost after iteration 700: 0.104507\n",
      "Cost after iteration 800: 0.095149\n",
      "Cost after iteration 900: 0.087561\n",
      "Cost after iteration 1000: 0.081408\n",
      "Cost after iteration 1100: 0.076418\n",
      "Cost after iteration 1200: 0.072371\n",
      "Cost after iteration 1300: 0.069090\n",
      "Cost after iteration 1400: 0.066428\n",
      "Cost after iteration 1500: 0.064269\n",
      "Cost after iteration 1600: 0.062519\n",
      "Cost after iteration 1700: 0.061098\n",
      "Cost after iteration 1800: 0.059947\n",
      "Cost after iteration 1900: 0.059012\n",
      "Cost after iteration 2000: 0.058254\n",
      "Cost after iteration 2100: 0.057639\n",
      "Cost after iteration 2200: 0.057141\n",
      "Cost after iteration 2300: 0.056736\n",
      "Cost after iteration 2400: 0.056408\n",
      "Cost after iteration 2500: 0.056141\n",
      "Cost after iteration 2600: 0.055925\n",
      "Cost after iteration 2700: 0.055750\n",
      "Cost after iteration 2800: 0.055607\n",
      "Cost after iteration 2900: 0.055492\n",
      "Cost after iteration 3000: 0.055398\n",
      "Cost after iteration 3100: 0.055322\n",
      "Cost after iteration 3200: 0.055260\n",
      "Cost after iteration 3300: 0.055210\n",
      "Cost after iteration 3400: 0.055169\n",
      "Cost after iteration 3500: 0.055136\n",
      "Cost after iteration 3600: 0.055110\n",
      "Cost after iteration 3700: 0.055088\n",
      "Cost after iteration 3800: 0.055070\n",
      "Cost after iteration 3900: 0.055056\n",
      "Cost after iteration 4000: 0.055044\n",
      "Cost after iteration 4100: 0.055035\n",
      "Cost after iteration 4200: 0.055027\n",
      "Cost after iteration 4300: 0.055021\n",
      "Cost after iteration 4400: 0.055016\n",
      "Cost after iteration 4500: 0.055012\n",
      "Cost after iteration 4600: 0.055008\n",
      "Cost after iteration 4700: 0.055005\n",
      "Cost after iteration 4800: 0.055003\n",
      "Cost after iteration 4900: 0.055001\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X,Y, num_iterations = 5000, print_cost= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Make your predictions!\n",
    "\n",
    "Now that you have the fitted parameters, you are able to predict any value with your neural network! You just need to perform the following computation:\n",
    "\n",
    "$$ Z = W X + b$$ \n",
    "\n",
    "Where $W$ and $b$ are in the parameters dictionary.\n",
    "\n",
    "<a name=\"ex07\"></a>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(X, parameters):\n",
    "\n",
    "    W = parameters['W']\n",
    "    b = parameters['b']\n",
    "\n",
    "    Z = np.dot(W, X) + b\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_hat = predict(X,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['y_hat'] = y_hat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check some predicted values versus the original ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[ 0.54099273  0.36745281  0.17642235  0.15291907  0.69640498 -0.06619126\n",
      "   0.8630934   0.33431738  0.45998803  0.59224306  1.34927822  0.00237099\n",
      "   0.47554181  0.12513148  1.00854926  0.02889406  0.27234619  0.41901051\n",
      "   0.57903619  0.95981531  0.57879422  0.70338819  0.80321414  1.10090243\n",
      "   1.01250042  0.51151777  0.81466114  0.11257742  0.60725144  1.02082811\n",
      "   0.73623707  0.21105812  0.26944469  0.3843581   0.26451475  0.42070141\n",
      "   0.38451211  0.49853852  0.53365968  0.81750432  0.36305137  0.09414849\n",
      "  -0.07555741  1.24293968 -0.02048062 -0.0785237   0.50591098  1.40933491\n",
      "   0.88266049  0.40391156  0.56063211  0.04732428  0.45933166  0.05659851\n",
      "   0.42864718  0.94232361  0.69824718  1.14672812  0.38355304  0.85885977\n",
      "   0.52705001  0.83322314  1.01028217  0.57680154  0.24778455  1.49066605\n",
      "   0.99678032  0.96869685  0.77611733  0.15688588  0.00834056  0.54385618\n",
      "   0.65922248  1.22724567  0.64485437  0.09198853  0.53065401  0.71368334\n",
      "   0.53983719  1.00776168  0.65568619  0.30936253  0.2953623   0.61852759\n",
      "   0.86718378  0.54811592  0.77340937  0.43268349  0.59117482  0.20514485\n",
      "   0.70436222  0.49600369  0.54618718  0.7732871   0.64782229  0.90687898\n",
      "   0.59254437  0.78069718  0.234389    1.0610725   0.13046717  0.98406932\n",
      "   0.39560706  0.55655724 -0.14552171  0.43985333  0.81441151  0.24155054\n",
      "   0.11174705  0.89873135  0.22719025  0.86112349  0.95207786  0.11857438\n",
      "   0.13279932 -0.06909339  0.8176562  -0.18743202  1.03579984  0.48199384\n",
      "   0.61358371 -0.14782089  1.16560851  1.05548267  0.23039941  0.910983\n",
      "   1.1711159   0.59000343  0.37671244  0.38326459  0.20205165  0.53666418\n",
      "   0.17262318  0.6170004   0.69619209  0.4753248   0.51760077  0.33363396\n",
      "   0.51469287  0.43320189  0.71678041  0.76177672  0.48787416  0.93898287\n",
      "   0.40862338  0.74336993  0.06903488  0.41370232  0.4591269   0.66902285\n",
      "   0.79229837  0.0756273   1.20279904  1.42770067  0.57332529  0.42790315\n",
      "   0.75162362  0.49933948  0.48769841 -0.12146552  0.35271102 -0.45080484\n",
      "   0.4768403   0.19529114  0.83199626  0.35424137  0.71228951  0.9177961\n",
      "   0.86494739  0.07754315 -0.13023499  0.79352587  1.32293587  0.66837697\n",
      "   0.30957377  0.53952098  0.96936603  0.43797457  0.74744366  0.54966245\n",
      "   0.27012337  0.35978211  0.36220511  1.5056299   0.97145663  0.00378973\n",
      "   0.79007765  0.745436    0.78743064  0.53702733  0.61092678  0.5785806\n",
      "   0.9254009   0.8625625   0.63706715  0.03975722  0.94305157  0.94576555\n",
      "   0.45089192  0.69562569  0.21376425  1.0555561   0.35199495  1.42418008\n",
      "   0.03490874  0.04236582 -0.27085361  0.60173221  0.12242437  0.69115506\n",
      "   0.28387845 -0.42477281  0.82194375  0.38825974  0.25254054  1.13845095\n",
      "   0.61906184  0.60381859  0.98889986  0.50946545  0.4496202   0.89779183\n",
      "   0.82667279  0.55683151 -0.13227358  1.26931999  0.39839993  0.47456728\n",
      "   0.29500514  0.72819653  0.83064486  0.75930471  1.53475391  0.86706248\n",
      "   0.39400346 -0.07206921  0.30456987  0.16643331 -0.61467036  0.44672322\n",
      "   0.40259254  0.68862718  0.13246676 -0.01665834  0.24942393  0.52637921\n",
      "   0.82571382  0.40073519  1.10245405 -0.01974193  0.81089151  0.45828277\n",
      "   0.07405138  0.94750754  0.77077532  0.06383323  0.85094705  0.50783109\n",
      "   0.62929051  0.29982016  0.93617283 -0.18633438  0.46917929  0.84366072\n",
      "   0.71130877  0.38387163  0.72505681  0.65393523  0.06251228  0.2272484\n",
      "   0.74865175  1.13117525  0.97025924  0.79045143  0.53814351  0.24590689\n",
      "  -0.09012937  0.62896089  0.58755856  0.41931157  0.47559345  0.12472374\n",
      "   0.45869933  0.90292696  0.84331753  0.09538561  0.32519851  0.58332672\n",
      "   0.06480071  0.75983231 -0.05086949  0.59686375  0.76455511  0.34640856\n",
      "   0.77428159  0.13268639  0.9144646   0.56317772  0.4396058   0.60291293\n",
      "   1.29618688  0.31089905 -0.07306895  0.2403205   0.42194634  0.91086851\n",
      "   0.33694744  0.79301674  0.74611832  0.74513561  0.69013746  0.34105274\n",
      "   0.93873192  0.19870641  1.95657223  0.81120112  0.99098827  0.45194685\n",
      "   0.24024053  0.2747454   0.57348283  0.51428619  0.56966664  0.12380883\n",
      "   0.42675419  0.76702282  0.47039777  0.96598031  0.45196174  0.69119516\n",
      "   0.67982292  0.97500659  0.23757302  0.52072032  0.88536367  0.58026027\n",
      "  -0.15002224  0.03200318 -0.07150934  0.11958182  0.78232338  1.04405126\n",
      "   0.54976215  0.24401077  0.64331583  1.097779    0.26822459  0.21920703\n",
      "   0.59584175  0.45259919  0.9322585   0.65696512  0.45926343  1.00375648\n",
      "   0.49654764  0.98083088  0.81664565  0.55796459  0.5518419   0.24918977\n",
      "   0.12377856  0.82448469  0.10803807  0.27074471  0.32627357  0.63084314\n",
      "   0.73077494 -0.31144349  0.79484199  0.63620827  0.79318404 -0.03499487\n",
      "   1.1719654   0.76239155  1.04141215  0.84053965  1.26414803  0.12090546\n",
      "   0.078339    0.91403344  0.83506159  0.79466054  0.12089301  0.41861357\n",
      "   0.50089864  0.38015168  0.37519544  0.89993776  0.32431501  0.12512578\n",
      "   0.22777248  0.96240987 -0.3724119   0.77396777 -0.14817183  0.88614022\n",
      "  -0.23545188  0.41189011  0.63591644  0.73157285  0.04964702  0.59430548\n",
      "   0.77657073  1.04880304  0.39627765  0.33796078  0.41126503  0.14139737\n",
      "   1.0976418   0.71397318  0.73457031  1.35747308  0.74695001  0.59417766\n",
      "   0.04437679  0.20640872  0.25758023  0.9057703   0.50917097 -0.34913552\n",
      "   0.70489202  0.8278164   0.76347168  0.46558056  0.86646211  0.49658991\n",
      "   0.31821827  0.4011687   0.6521963   0.2839857   1.26369988  1.04543038\n",
      "   0.53436243  0.48896508  0.7822005   0.11737132  0.88376213  1.04500368\n",
      "   0.97683194  0.90219927  0.91394439  0.00460508  0.43544645  1.34801848\n",
      "   0.46565521  0.27418212  0.20777076  0.95875676  0.72074115  0.64144658\n",
      "   0.32751293  0.88925235  0.81710476  0.5501712   0.58859486  0.16727478\n",
      "   0.54691245  0.81950888  0.46149173  0.63938524  0.45315863  0.37787388\n",
      "   0.73211088  0.35401682  0.17924703  0.51076931  0.59657378  0.84606343\n",
      "   0.84215505  0.15919162  0.76466194  0.48444782  0.25722465  0.46319498\n",
      "   0.7704707   0.46892917 -0.68286572  0.02825702  0.74514172  0.27318675\n",
      "   0.80952154 -0.02902989  0.35399699  0.69506241  0.87983975  0.63051282\n",
      "   0.90287012  1.22602206  0.67688038  0.16669205  0.53316675  0.66614456\n",
      "   0.50393489  0.8264749   0.40872056  0.26417722  0.09900861 -0.11310462\n",
      "   0.30465032  0.47050844]]\n",
      "True Labels: [[0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0\n",
      "  1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1\n",
      "  1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0\n",
      "  0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1\n",
      "  0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
      "  0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1\n",
      "  1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1\n",
      "  0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0\n",
      "  0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0\n",
      "  1 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0\n",
      "  0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1\n",
      "  0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1\n",
      "  0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
      "  1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Test the neural network\n",
    "predictions = predict(X, parameters)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"True Labels:\", Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
